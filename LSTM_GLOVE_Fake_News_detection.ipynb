{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stance_Detection_for_the_Fake_News_Challenge_Esquence_Models_In_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9jhXKPCFcJ",
        "colab_type": "text"
      },
      "source": [
        "#Fake News Detection\n",
        "\n",
        "## Identifying Textual Relationships with Deep Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AS39z1XgFpT",
        "colab_type": "code",
        "outputId": "89ee8b0f-eaae-4462-cc7f-f78386b7b85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhZdJ4zpwWzN",
        "colab_type": "text"
      },
      "source": [
        "#### Path for Project files on google drive\n",
        "\n",
        "**Note:** You need to change this path according where you have kept the files in google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_7yCFdzgFsH",
        "colab_type": "code",
        "outputId": "be3656eb-e8eb-4f67-e8bd-bb425b9eed3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set current working directory so that all relative path references work\n",
        "\n",
        "root = \"/content/drive/My Drive/PGP-AIML/Fake News\"\n",
        "\n",
        "%cd $root"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PGP-AIML/Fake News\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQnYq4hVO4ft",
        "colab_type": "code",
        "outputId": "777d2a2d-e0b4-47c3-f201-ad87c065d09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from os import listdir, makedirs\n",
        "from os.path import isfile, join, exists\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import itertools\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ly0VxAnwJ2f",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmsPn6PF-cgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from zipfile import ZipFile\n",
        "# with ZipFile(join(root, 'glove.6B.zip'), 'r') as z:\n",
        "#   z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjLJEQ_PwcGi",
        "colab_type": "text"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "1. Using [read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) in pandas load the given train datasets files **`train_bodies.csv`** and **`train_stances.csv`**\n",
        "\n",
        "2. Using [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) command in pandas merge the two datasets based on the Body ID. \n",
        "\n",
        "Note: Save the final merged dataset in a dataframe with name **`dataset`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXO1WZ-gFwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bodies = pd.read_csv(\"train_bodies.csv\")\n",
        "train_stances = pd.read_csv(\"train_stances.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rQxqXWATjDv",
        "colab_type": "code",
        "outputId": "5bc82a4f-7c11-4fe6-e39d-1e55651a0460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(\"TRAIN BODIES\\n\" + '-' * 12)\n",
        "print(\"Shape   : {}\".format(train_bodies.shape))\n",
        "print(\"Columns : [ {} ]\".format(\", \".join(train_bodies.columns)))\n",
        "print(\"\")\n",
        "print(\"TRAIN STANCES\\n\" + '-' * 13)\n",
        "print(\"Shape   : {}\".format(train_stances.shape))\n",
        "print(\"Columns : [ {} ]\".format(\", \".join(train_stances.columns)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN BODIES\n",
            "------------\n",
            "Shape   : (1683, 2)\n",
            "Columns : [ Body ID, articleBody ]\n",
            "\n",
            "TRAIN STANCES\n",
            "-------------\n",
            "Shape   : (49972, 3)\n",
            "Columns : [ Headline, Body ID, Stance ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kosAWskdOOT8",
        "colab_type": "code",
        "outputId": "9cf82a3d-32a6-47f7-8cbc-e9a5201a1750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_df = train_bodies.merge(train_stances, on='Body ID')\n",
        "\n",
        "print(\"MERGED TRAIN DATA\\n\" + '-' * 17)\n",
        "print(\"Shape   : {}\".format(train_df.shape))\n",
        "print(\"Columns : [ {} ]\".format(\", \".join(train_df.columns)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MERGED TRAIN DATA\n",
            "-----------------\n",
            "Shape   : (49972, 4)\n",
            "Columns : [ Body ID, articleBody, Headline, Stance ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5lrVRt7WcKD",
        "colab_type": "code",
        "outputId": "1ba572c3-55ab-492b-88fb-fa5556217bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody  \\\n",
              "0        0  A small meteorite crashed into a wooded area i...   \n",
              "1        0  A small meteorite crashed into a wooded area i...   \n",
              "2        0  A small meteorite crashed into a wooded area i...   \n",
              "3        0  A small meteorite crashed into a wooded area i...   \n",
              "4        0  A small meteorite crashed into a wooded area i...   \n",
              "\n",
              "                                            Headline     Stance  \n",
              "0  Soldier shot, Parliament locked down after gun...  unrelated  \n",
              "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
              "2  Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
              "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
              "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDXSdpvqjuqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SENTS = 20\n",
        "MAX_SENTS_HEADING = 1\n",
        "MAX_SENT_LENGTH = 20\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqwm_GbwwnhX",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing the text and loading the pre-trained Glove word embeddings for each token "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-VUgh2yoMlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eErmmVJZMolT",
        "colab_type": "code",
        "outputId": "b2fef293-33ed-43a9-d19e-09da1dfeaa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "tmp = Tokenizer(num_words=4, oov_token='OUT_OF_VOCAB')\n",
        "\n",
        "tmp_texts = [\"This is some text to test this tokenizer thing! I heard the tokenizer thing is pretty cool.\"]\n",
        "\n",
        "tmp.fit_on_texts(tmp_texts)\n",
        "\n",
        "tmp.word_index\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OUT_OF_VOCAB': 1,\n",
              " 'cool': 14,\n",
              " 'heard': 11,\n",
              " 'i': 10,\n",
              " 'is': 3,\n",
              " 'pretty': 13,\n",
              " 'some': 6,\n",
              " 'test': 9,\n",
              " 'text': 7,\n",
              " 'the': 12,\n",
              " 'thing': 5,\n",
              " 'this': 2,\n",
              " 'to': 8,\n",
              " 'tokenizer': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TR6uPF0Moh5",
        "colab_type": "code",
        "outputId": "9b9d5150-76b9-4a90-c9d3-1beaac886ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "tmp_words = text_to_word_sequence(tmp_texts[0])\n",
        "\n",
        "tmp_words\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'some',\n",
              " 'text',\n",
              " 'to',\n",
              " 'test',\n",
              " 'this',\n",
              " 'tokenizer',\n",
              " 'thing',\n",
              " 'i',\n",
              " 'heard',\n",
              " 'the',\n",
              " 'tokenizer',\n",
              " 'thing',\n",
              " 'is',\n",
              " 'pretty',\n",
              " 'cool']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9AnUcAPMoX1",
        "colab_type": "code",
        "outputId": "ef2138b9-b8b2-4fde-ed9c-f024705cf21c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(itertools.chain.from_iterable(tmp.texts_to_sequences(tmp_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm85qirPofc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBe1KuXDosJ7",
        "colab_type": "text"
      },
      "source": [
        "#### Now, using fit_on_texts() from Tokenizer class, lets encode the data \n",
        "\n",
        "Note: We need to fit articleBody and Headline also to cover all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5rk-UyBlmyA",
        "colab_type": "code",
        "outputId": "e4d1612e-7d65-480b-efad-ff67518eadf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "texts = []\n",
        "\n",
        "texts = train_df['articleBody'].tolist() + train_df['Headline'].tolist()\n",
        "print(len(texts))\n",
        "\n",
        "tokenizer.fit_on_texts(texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXhiWGuq3mhU",
        "colab_type": "code",
        "outputId": "c38f7198-b4df-4628-a676-59818f646856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zYfXEq_3mVe",
        "colab_type": "code",
        "outputId": "f9724910-5119-4ce7-bd92-f4bcf05e49da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.document_count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omptHX-JpBsN",
        "colab_type": "text"
      },
      "source": [
        "#### fit_on_texts() gives the following attributes in the output as given [here](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/).\n",
        "\n",
        "* **word_counts:** dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_docs:** dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_index:** dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n",
        "\n",
        "* **document_count:** int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwE7CPHdiDT-",
        "colab_type": "text"
      },
      "source": [
        "### Download the `Punkt` from nltk using the commands given below. This is for sentence tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsiKmyJUZ-hU",
        "colab_type": "code",
        "outputId": "b296ce20-e5a1-44c4-e77d-06896c946447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHnsT2sTtFAA",
        "colab_type": "text"
      },
      "source": [
        "### Now, tokenize the sentences using nltk sent_tokenize() and encode the senteces with the ids we got form the above `t.word_index`\n",
        "\n",
        "Initialise 2 lists with names `texts` and `articles`.\n",
        "\n",
        "```\n",
        "texts = [] to store text of article as it is.\n",
        "\n",
        "articles = [] split the above text into a list of sentences.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctEu-d4c4EZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_texts = train_df['articleBody'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwWNa2PB7FE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_sents = []\n",
        "\n",
        "for article in article_texts:\n",
        "  article_sents.append(nltk.sent_tokenize(article))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xYykSnv7yXN",
        "colab_type": "code",
        "outputId": "03758a8f-4d00-4dc8-af87-0a5d744fec8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "article_texts[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGrQtr_p7x_z",
        "colab_type": "code",
        "outputId": "f34febe8-3e9f-4ebf-cc95-a61ed354ecad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "article_sents[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday.\",\n",
              " \"Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports.\",\n",
              " 'Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\"',\n",
              " 'House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports.',\n",
              " 'Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.',\n",
              " 'The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee.',\n",
              " 'He said it is still not clear if the meteorite disintegrated or was buried.',\n",
              " 'Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.',\n",
              " '\"We have to study it more because it could be ice or rock,\" he said.',\n",
              " 'Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light.',\n",
              " 'We have to ask if anyone has a photo or something.\"',\n",
              " \"Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\",\n",
              " '\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast.',\n",
              " 'We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.',\n",
              " \"The site of the crater is near Managua's international airport and an air force base.\",\n",
              " 'Only journalists from state media were allowed to visit it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVyClBULCqWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Slicing using an index greater than the length of a list just returns the entire list.\n",
        "# Multiplying a list by a negative value returns an empty list.\n",
        "def trim_or_pad(lst, n):\n",
        "  return lst[:n] + [0]*(n-len(lst))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oCo4dsyIv_7",
        "colab_type": "code",
        "outputId": "71e4b903-4ffc-4994-df81-da3269650db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "article_encodings = []\n",
        "\n",
        "for i in range(len(article_sents)):\n",
        "  sentence_encodings = []\n",
        "  n_sents = 0\n",
        "  for sentence in article_sents[i]:\n",
        "    # Limit number of sentences to MAX_SENTS\n",
        "    if(n_sents < MAX_SENTS):\n",
        "      print(\"Working on article no. {}, sentence no. {}\".format(i, n_sents), end=\"\\r\")\n",
        "      n_sents += 1\n",
        "\n",
        "      words = text_to_word_sequence(sentence)\n",
        "      word_encodings = list(itertools.chain.from_iterable(tokenizer.texts_to_sequences(words)))\n",
        "      word_encodings = trim_or_pad(word_encodings, MAX_SENT_LENGTH)\n",
        "      \n",
        "      sentence_encodings.append(word_encodings)\n",
        "    else:\n",
        "      break\n",
        "    \n",
        "  # Handle the case where an article has number of sentences less than MAX_SENTS \n",
        "  while(n_sents < MAX_SENTS):\n",
        "    print(\"Working on article no. {}, sentence no. {}\".format(i, n_sents), end=\"\\r\")\n",
        "    n_sents += 1\n",
        "\n",
        "    empty_encodings = trim_or_pad([], MAX_SENT_LENGTH)\n",
        "    sentence_encodings.append(empty_encodings)\n",
        "\n",
        "  article_encodings.append(sentence_encodings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMf3AEdmIvyh",
        "colab_type": "code",
        "outputId": "bad6fb9c-8c97-4253-d78d-14807031ae9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "article_encodings = np.array(article_encodings)\n",
        "article_encodings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49972, 20, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mtM2jQnIvu8",
        "colab_type": "code",
        "outputId": "4ee9c855-57d0-4f67-862b-c110ecb69517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "source": [
        "article_encodings[0, :, :]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    3,   481,   427,  7211,    81,     3,  3733,   331,     5,\n",
              "         3891,   350,     4,  1431,  2958,     1,    89,    12,   464,\n",
              "            0,     0],\n",
              "       [  758,    95,  1047,     3,  2679,  1752,     7,   189,     3,\n",
              "         1217,  1075,  2030,   700,   159,     1,  3032,   448,     1,\n",
              "          555,   235],\n",
              "       [   89,  1067,  4115,  2349,    12,     3,  1092,  3306,    19,\n",
              "            1,    89,     2,  1793,     1,   521,  2009,    15,     9,\n",
              "            3,  3111],\n",
              "       [  181,  3640,   972,   200,  2556,    44,  6775,  1722,  1252,\n",
              "            5, 13317, 17936,     1,   778,    31,   740,  3990,    67,\n",
              "           85,     0],\n",
              "       [ 2349,    12,  1557,    38,  1094,   351,   775,     2,   367,\n",
              "          260,  1770,     5,  4450,    70,   494,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    1,   700,   189,    19,     1,   427,    32,     3,  7417,\n",
              "            4,  2159,  1252,     6,     3,  5270,     4,  1217,  1252,\n",
              "           12,  3363],\n",
              "       [   13,    12,    15,     8,   149,    25,   543,    64,     1,\n",
              "          427,  3727,    41,     9,  1850,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [ 3363,  5733,     4,     1,  5875,   614,    21,     1,   311,\n",
              "         3438,   794,     4,  1557,    12,     1,   427,    69,    23,\n",
              "          787,     2],\n",
              "       [   37,    17,     2,  1793,    15,    52,   120,    15,    69,\n",
              "           23,  4921,    41,  1963,    13,    12,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [ 4736,  3338,    24,  3969,     2,     1,  1316,     4,  3072,\n",
              "         1653,    12,    15,     9,   195,  1420,     7,    58,    40,\n",
              "           95,     3],\n",
              "       [   37,    17,     2,  1094,    64,   510,    20,     3,   250,\n",
              "           41,   264,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  260,   758,    95,  1047,     3,  1806,  1752,   531,   276,\n",
              "           29,    12,    33,   703,   163,   892,  1420,     5,     1,\n",
              "         2081,     0],\n",
              "       [   35,     9,  2057,    10,   116,  5825,     6,    35,   576,\n",
              "          656,   104,    59,     4,     3,  2410,    35,   241,     3,\n",
              "          512,  1911],\n",
              "       [   37,   341,    15,     9,     3,  2082,   120,    37,   881,\n",
              "           24,  4451,  2584,  4315,  4922,    55,     1,   555,   235,\n",
              "            0,     0],\n",
              "       [    1,   255,     4,     1,   700,     8,   159,  3961,   351,\n",
              "          448,     6,    24,   155,   465,  1929,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  126,   921,    22,    47,   100,    36,  1833,     2,  1212,\n",
              "           15,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egPOnrYYIvsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline_texts = train_df['Headline'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMg5DQm4Ivo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline_sents = []\n",
        "\n",
        "for headline in headline_texts:\n",
        "  headline_sents.append(nltk.sent_tokenize(headline))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CliiIhLemJV",
        "colab_type": "code",
        "outputId": "cfb10f6b-b128-49ef-a48b-15c99df098bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "headline_encodings = []\n",
        "\n",
        "for i in range(len(headline_sents)):\n",
        "  sentence_encodings = []\n",
        "  n_sents = 0\n",
        "  for sentence in headline_sents[i]:\n",
        "    # Limit number of sentences to MAX_SENTS_HEADING\n",
        "    if(n_sents < MAX_SENTS_HEADING):\n",
        "      print(\"Working on headline no. {}, sentence no. {}\".format(i, n_sents), end=\"\\r\")\n",
        "      n_sents += 1\n",
        "\n",
        "      words = text_to_word_sequence(sentence)\n",
        "      word_encodings = list(itertools.chain.from_iterable(tokenizer.texts_to_sequences(words)))\n",
        "      word_encodings = trim_or_pad(word_encodings, MAX_SENT_LENGTH)\n",
        "      \n",
        "      sentence_encodings.append(word_encodings)\n",
        "    else:\n",
        "      break\n",
        "    \n",
        "  # Handle the case where an headline has number of sentences less than MAX_SENTS_HEADING \n",
        "  while(n_sents < MAX_SENTS_HEADING):\n",
        "    print(\"Working on headline no. {}, sentence no. {}\".format(i, n_sents), end=\"\\r\")\n",
        "    n_sents += 1\n",
        "\n",
        "    empty_encodings = trim_or_pad([], MAX_SENT_LENGTH)\n",
        "    sentence_encodings.append(empty_encodings)\n",
        "\n",
        "  headline_encodings.append(sentence_encodings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm49cWU3sU4E",
        "colab_type": "code",
        "outputId": "881132de-b704-4206-9936-b2bcff941214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "headline_encodings = np.array(headline_encodings)\n",
        "headline_encodings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49972, 1, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYTsvG6csfiN",
        "colab_type": "code",
        "outputId": "7a2026a3-a4ab-4f92-d4cc-870c1a35a361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "headline_encodings[0, :, :]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  718,   206,   343,  7134,   193,    34,  1338, 11554,    21,\n",
              "          233,   686,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaH0Ey1qe_Co",
        "colab_type": "text"
      },
      "source": [
        "### Now the features are ready, lets make the labels ready for the model to process.\n",
        "\n",
        "### Convert labels into one-hot vectors\n",
        "\n",
        "You can use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) in pandas to create one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq-VcgM8fat1",
        "colab_type": "code",
        "outputId": "787a1263-d5eb-46ae-a62b-a2031e7e915b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels = pd.get_dummies(train_df['Stance'])\n",
        "labels.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['agree', 'disagree', 'discuss', 'unrelated'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y90ju-5EtUxL",
        "colab_type": "code",
        "outputId": "8eeaf0e0-0ea4-4234-f784-f5482e38cf4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels = labels.values\n",
        "labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49972, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6LvNqyvteGa",
        "colab_type": "code",
        "outputId": "edf430e3-5189-4803-a7c9-8f2254695517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Shape of articles tensor : {}\".format(article_encodings.shape))\n",
        "print(\"Shape of headlines tensor : {}\".format(headline_encodings.shape))\n",
        "print(\"Shape of labels tensor : {}\".format(labels.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of articles tensor : (49972, 20, 20)\n",
            "Shape of headlines tensor : (49972, 1, 20)\n",
            "Shape of labels tensor : (49972, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDOxHdR3frDu",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ra-yYTvfzRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get numbers upto no.of articles\n",
        "indices = np.arange(article_encodings.shape[0])\n",
        "## shuffle the numbers\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKnSqwIFf3Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## shuffle the data\n",
        "article_encodings = article_encodings[indices]\n",
        "\n",
        "headline_encodings = headline_encodings[indices]\n",
        "\n",
        "labels = labels[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcOFVfPBf9kA",
        "colab_type": "text"
      },
      "source": [
        "### Split into train and validation sets. Split the train set 80:20 ratio to get the train and validation sets.\n",
        "\n",
        "\n",
        "Use the variable names as given below:\n",
        "\n",
        "x_train, x_val - for body of articles.\n",
        "\n",
        "x-heading_train, x_heading_val - for heading of articles.\n",
        "\n",
        "y_train - for training labels.\n",
        "\n",
        "y_val - for validation labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2neh9Wcof8iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(article_encodings, labels, test_size=0.2, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5u3PTz3gEV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_headline_train, X_headline_val, y_headline_train, y_headline_val = train_test_split(headline_encodings, labels, test_size=0.2, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHEFLrfPETe-",
        "colab_type": "code",
        "outputId": "c5b27456-b6dc-4dc4-c918-1f4b9c675a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], MAX_SENTS * MAX_SENT_LENGTH))\n",
        "print(X_train.shape)\n",
        "\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], MAX_SENTS * MAX_SENT_LENGTH))\n",
        "print(X_val.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39977, 400)\n",
            "(9995, 400)\n",
            "(39977, 4)\n",
            "(9995, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOQovzw_EfBq",
        "colab_type": "code",
        "outputId": "c8c6a799-818a-4184-833f-1179ea8c0883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X_headline_train = np.reshape(X_headline_train, (X_headline_train.shape[0], MAX_SENTS_HEADING * MAX_SENT_LENGTH))\n",
        "print(X_headline_train.shape)\n",
        "\n",
        "X_headline_val = np.reshape(X_headline_val, (X_headline_val.shape[0], MAX_SENTS_HEADING * MAX_SENT_LENGTH))\n",
        "print(X_headline_val.shape)\n",
        "\n",
        "print(y_headline_train.shape)\n",
        "print(y_headline_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39977, 20)\n",
            "(9995, 20)\n",
            "(39977, 4)\n",
            "(9995, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNnoBtArhJ1E",
        "colab_type": "text"
      },
      "source": [
        "### Create embedding matrix with the glove embeddings\n",
        "\n",
        "\n",
        "Run the below code to create embedding_matrix which has all the words and their glove embedding if present in glove word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKqn2IL2ZF8v",
        "colab_type": "code",
        "outputId": "a0fcb9f8-6bd9-4d83-bae6-f609f814efed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('./glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((MAX_NB_WORDS, 100))\n",
        "\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if i >= MAX_NB_WORDS: \n",
        "    continue\n",
        "\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRi4o3ZspDFU",
        "colab_type": "text"
      },
      "source": [
        "## Try a bidirectional LSTM model and report the accuracy score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpkVhIbx3gr1",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_8QXh-rmPFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ = Input(shape=(MAX_SENTS * MAX_SENT_LENGTH, ))\n",
        "x = Embedding(MAX_NB_WORDS, 100, weights=[embedding_matrix])(input_) # embed size is 100 given that we've used glove.6B.100d.txt\n",
        "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(100, activation=\"relu\")(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(4, activation=\"softmax\")(x)\n",
        "model = Model(inputs=input_, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67B2UMNVKGu0",
        "colab_type": "code",
        "outputId": "91e08069-7f27-414f-b522-aec77090202a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 400, 100)          2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 400, 200)          160800    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 2,181,304\n",
            "Trainable params: 2,181,304\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Xrd-JQ3id7",
        "colab_type": "text"
      },
      "source": [
        "### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlduHU2CovxC",
        "colab_type": "code",
        "outputId": "c73ad784-5501-426e-8aca-c2e6f6951b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39977 samples, validate on 9995 samples\n",
            "Epoch 1/5\n",
            "39977/39977 [==============================] - 1364s 34ms/step - loss: 0.3256 - acc: 0.8698 - val_loss: 0.2927 - val_acc: 0.8800\n",
            "Epoch 2/5\n",
            "39977/39977 [==============================] - 1363s 34ms/step - loss: 0.2769 - acc: 0.8893 - val_loss: 0.2634 - val_acc: 0.8992\n",
            "Epoch 3/5\n",
            "39977/39977 [==============================] - 1353s 34ms/step - loss: 0.2565 - acc: 0.8982 - val_loss: 0.2501 - val_acc: 0.9040\n",
            "Epoch 4/5\n",
            "39977/39977 [==============================] - 1352s 34ms/step - loss: 0.2452 - acc: 0.9031 - val_loss: 0.2431 - val_acc: 0.9048\n",
            "Epoch 5/5\n",
            "39977/39977 [==============================] - 1357s 34ms/step - loss: 0.2362 - acc: 0.9066 - val_loss: 0.2386 - val_acc: 0.9084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efb7c4edf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_btKAc6tNkR",
        "colab_type": "code",
        "outputId": "b2f3791a-004c-49a9-d624-090e47d2a0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "input_ = Input(shape=(MAX_SENTS_HEADING * MAX_SENT_LENGTH, ))\n",
        "x = Embedding(MAX_NB_WORDS, 100, weights=[embedding_matrix])(input_) # embed size is 100 given that we've used glove.6B.100d.txt\n",
        "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(100, activation=\"relu\")(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(4, activation=\"softmax\")(x)\n",
        "model = Model(inputs=input_, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKjNE6jNtqW4",
        "colab_type": "code",
        "outputId": "c40f4f6e-9842-4a3b-d659-29352e8732e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 20, 100)           2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 20, 200)           160800    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 2,181,304\n",
            "Trainable params: 2,181,304\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AMrzMXZttO3",
        "colab_type": "code",
        "outputId": "0ac7f25b-488d-4f6b-ac1e-94043db664e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "model.fit(X_headline_train, y_headline_train, batch_size=64, epochs=5, validation_data=(X_headline_val, y_headline_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 39977 samples, validate on 9995 samples\n",
            "Epoch 1/5\n",
            "39977/39977 [==============================] - 92s 2ms/step - loss: 0.3464 - acc: 0.8649 - val_loss: 0.3348 - val_acc: 0.8642\n",
            "Epoch 2/5\n",
            "39977/39977 [==============================] - 90s 2ms/step - loss: 0.3297 - acc: 0.8658 - val_loss: 0.3279 - val_acc: 0.8642\n",
            "Epoch 3/5\n",
            "39977/39977 [==============================] - 97s 2ms/step - loss: 0.3207 - acc: 0.8657 - val_loss: 0.3260 - val_acc: 0.8642\n",
            "Epoch 4/5\n",
            "39977/39977 [==============================] - 97s 2ms/step - loss: 0.3148 - acc: 0.8656 - val_loss: 0.3263 - val_acc: 0.8632\n",
            "Epoch 5/5\n",
            "39977/39977 [==============================] - 96s 2ms/step - loss: 0.3111 - acc: 0.8657 - val_loss: 0.3269 - val_acc: 0.8642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f874ac1be80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PsIwWwhtXwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}